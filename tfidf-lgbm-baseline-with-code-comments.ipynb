{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5899b454",
   "metadata": {
    "papermill": {
     "duration": 0.008988,
     "end_time": "2024-04-09T07:59:59.924788",
     "exception": false,
     "start_time": "2024-04-09T07:59:59.915800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Thank you for exploring my notebook.\n",
    "In this notebook, I used TfidfVectorizer and Polars to generate features, and used LightGBM as the scoring model.\n",
    "\n",
    "In addition, I used both Chinese and English as code comments.\n",
    "\n",
    "If you found my work useful, please feel free to give me a upvote. Thank you!\n",
    "##### update: version 2 \n",
    "1. Add more data Preprocessing function\n",
    "2. LGBMClassifier  -->  LGBMRegressor\n",
    "3. CV:  0.7646  --> 0.7871\n",
    "4. LB:  0.772 --> 0.786\n",
    "\n",
    "##### update: version 4\n",
    "1. Add code comments\n",
    "2. Using kappa as the early stop metric\n",
    "3. CV:  0.7889\n",
    "4. LB:  0.787\n",
    "\n",
    "##### update: version 5\n",
    "1. Using kappa as LGBMRegressor objective\n",
    "2. CV:  0.7990"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0816a9ac",
   "metadata": {
    "papermill": {
     "duration": 0.008104,
     "end_time": "2024-04-09T07:59:59.942401",
     "exception": false,
     "start_time": "2024-04-09T07:59:59.934297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789180c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T07:59:59.961653Z",
     "iopub.status.busy": "2024-04-09T07:59:59.960790Z",
     "iopub.status.idle": "2024-04-09T08:00:04.133703Z",
     "shell.execute_reply": "2024-04-09T08:00:04.132707Z"
    },
    "papermill": {
     "duration": 4.185744,
     "end_time": "2024-04-09T08:00:04.136587",
     "exception": false,
     "start_time": "2024-04-09T07:59:59.950843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "from tqdm.auto import tqdm,trange\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527e9c3",
   "metadata": {
    "papermill": {
     "duration": 0.008849,
     "end_time": "2024-04-09T08:00:04.155211",
     "exception": false,
     "start_time": "2024-04-09T08:00:04.146362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f930d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17307"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9c08f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T08:00:04.174460Z",
     "iopub.status.busy": "2024-04-09T08:00:04.173363Z",
     "iopub.status.idle": "2024-04-09T08:00:05.014276Z",
     "shell.execute_reply": "2024-04-09T08:00:05.012901Z"
    },
    "papermill": {
     "duration": 0.853773,
     "end_time": "2024-04-09T08:00:05.017215",
     "exception": false,
     "start_time": "2024-04-09T08:00:04.163442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        white-space: pre;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        padding-top: 0;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        padding-bottom: 0;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "<small>shape: (1, 4)</small>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>\n",
       "essay_id\n",
       "</th>\n",
       "<th>\n",
       "full_text\n",
       "</th>\n",
       "<th>\n",
       "score\n",
       "</th>\n",
       "<th>\n",
       "paragraph\n",
       "</th>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "str\n",
       "</td>\n",
       "<td>\n",
       "str\n",
       "</td>\n",
       "<td>\n",
       "i64\n",
       "</td>\n",
       "<td>\n",
       "list[str]\n",
       "</td>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>\n",
       "&quot;000d118&quot;\n",
       "</td>\n",
       "<td>\n",
       "&quot;Many people ha...\n",
       "</td>\n",
       "<td>\n",
       "3\n",
       "</td>\n",
       "<td>\n",
       "[&quot;Many people have car where they live. The thing they don&#x27;t know is that when you use a car alot of thing can happen like you can get in accidet or the smoke that the car has is bad to breath on if someone is walk but in VAUBAN,Germany they dont have that proble because 70 percent of vauban&#x27;s families do not own cars,and 57 percent sold a car to move there. Street parkig ,driveways and home garages are forbidden on the outskirts of freiburd that near the French and Swiss borders. You probaly won&#x27;t see a car in Vauban&#x27;s streets because they are completely &quot;car free&quot; but If some that lives in VAUBAN that owns a car ownership is allowed,but there are only two places that you can park a large garages at the edge of the development,where a car owner buys a space but it not cheap to buy one they sell the space for you car for $40,000 along with a home. The vauban people completed this in 2006 ,they said that this an example of a growing trend in Europe,The untile states and some where else are suburban life from auto use this is called &quot;smart planning&quot;. The current efforts to drastically reduce greenhouse gas emissions from tailes the passengee cars are responsible for 12 percent of greenhouse gas emissions in Europe and up to 50 percent in some car intensive in the United States. I honeslty think that good idea that they did that is Vaudan because that makes cities denser and better for walking and in VAUBAN there are 5,500 residents within a rectangular square mile. In the artical David Gold berg said that &quot;All of our development since World war 2 has been centered on the cars,and that will have to change&quot; and i think that was very true what David Gold said because alot thing we need cars to do we can go anyway were with out cars beacuse some people are a very lazy to walk to place thats why they alot of people use car and i think that it was a good idea that that they did that in VAUBAN so people can see how we really don&#x27;t need car to go to place from place because we can walk from were we need to go or we can ride bycles with out the use of a car. It good that they are doing that if you thik about your help the earth in way and thats a very good thing to. In the United states ,the Environmental protection Agency is promoting what is called &quot;car reduced&quot;communtunties,and the legislators are starting to act,if cautiously. Maany experts expect pubic transport serving suburbs to play a much larger role in a new six years federal transportation bill to approved this year. In previous bill,80 percent of appropriations have by law gone to highways and only 20 percent to other transports. There many good reason why they should do this.    &quot;]\n",
       "</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "shape: (1, 4)\n",
       "┌──────────┬─────────────────────────────────────┬───────┬─────────────────────────────────────┐\n",
       "│ essay_id ┆ full_text                           ┆ score ┆ paragraph                           │\n",
       "│ ---      ┆ ---                                 ┆ ---   ┆ ---                                 │\n",
       "│ str      ┆ str                                 ┆ i64   ┆ list[str]                           │\n",
       "╞══════════╪═════════════════════════════════════╪═══════╪═════════════════════════════════════╡\n",
       "│ 000d118  ┆ Many people have car where they ... ┆ 3     ┆ [\"Many people have car where the... │\n",
       "└──────────┴─────────────────────────────────────┴───────┴─────────────────────────────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [  \n",
    "    (\n",
    "        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n",
    "    ),\n",
    "]\n",
    "PATH = \"./\"\n",
    "# 载入训练集和测试集，同时对full_text数据使用\\n\\n字符分割为列表，重命名为paragraph\n",
    "# Load training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for full_text data\n",
    "train = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\n",
    "test = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n",
    "# 显示训练集中的第一个样本数据\n",
    "# Display the first sample data in the training set\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd593b",
   "metadata": {
    "papermill": {
     "duration": 0.008548,
     "end_time": "2024-04-09T08:00:05.034904",
     "exception": false,
     "start_time": "2024-04-09T08:00:05.026356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Features engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583cf3e",
   "metadata": {
    "papermill": {
     "duration": 0.008303,
     "end_time": "2024-04-09T08:00:05.051685",
     "exception": false,
     "start_time": "2024-04-09T08:00:05.043382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169dc49a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T08:00:05.070981Z",
     "iopub.status.busy": "2024-04-09T08:00:05.070238Z",
     "iopub.status.idle": "2024-04-09T08:00:05.078542Z",
     "shell.execute_reply": "2024-04-09T08:00:05.077664Z"
    },
    "papermill": {
     "duration": 0.021058,
     "end_time": "2024-04-09T08:00:05.081233",
     "exception": false,
     "start_time": "2024-04-09T08:00:05.060175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def removeHTML(x):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',x)\n",
    "def dataPreprocessing(x):\n",
    "    # 将单词转化为小写\n",
    "    # Convert words to lowercase\n",
    "    x = x.lower()\n",
    "    # Remove HTML\n",
    "    # 移除html\n",
    "    x = removeHTML(x)\n",
    "    # 删除以@作为首字母的字符串\n",
    "    # Delete strings starting with @\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    # 删除数字\n",
    "    # Delete Numbers\n",
    "    x = re.sub(\"'\\d+\", '',x)\n",
    "    x = re.sub(\"\\d+\", '',x)\n",
    "    # 删除网址\n",
    "    # Delete URL\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    # 将连续空白符替换为一个空格字符\n",
    "    # Replace consecutive empty spaces with a single space character\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    # 替换连续的句号和逗号为一个\n",
    "    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    # 去除开头结尾的空白符\n",
    "    # Remove empty characters at the beginning and end\n",
    "    x = x.strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b5b599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Many people have car where they live. The thing they don\\'t know is that when you use a car alot of thing can happen\\xa0like you can get in accidet or\\xa0the smoke that the car has is bad to breath\\xa0on if someone is walk but in VAUBAN,Germany they dont have that proble because 70 percent of vauban\\'s families do not own cars,and 57 percent sold a car to move there. Street parkig ,driveways and home garages are forbidden\\xa0on the outskirts of freiburd that near the French and Swiss borders. You probaly won\\'t see a car in Vauban\\'s streets because they are completely \"car free\" but\\xa0If some that lives in VAUBAN that owns a car ownership is allowed,but there are only two places that you can park a large garages at the edge of the development,where a car owner buys a space but it not cheap to buy one they sell the space for you car for $40,000 along with a home. The vauban people completed this in 2006 ,they said that this an example of a growing trend in Europe,The untile states and some where else are suburban life from auto use this is called \"smart planning\". The current efforts to drastically reduce greenhouse gas emissions from tailes the passengee cars are responsible for 12 percent of greenhouse gas emissions in Europe and up to 50 percent in some car intensive in the United States. I honeslty think that good idea that they did that is Vaudan because that makes cities denser and better for walking and in VAUBAN there are 5,500 residents within a rectangular square mile. In the artical David Gold berg said that \"All of our development since World war 2 has been centered on the cars,and that will have to change\" and i think that was very true what David Gold said because alot thing we need cars to do we can go anyway were with out cars beacuse some people are a very lazy to walk to place thats why they alot of people use car and i think that it was a good idea that that they did that in VAUBAN so people can see how we really don\\'t need car to go to place from place because we can walk from were we need to go or we can ride bycles with out the use of a car. It good that they are doing that if you thik about your help the earth in way and thats a very good thing to. In the United states ,the Environmental protection Agency is promoting what is called \"car reduced\"communtunties,and the legislators are starting to act,if cautiously. Maany experts expect pubic transport serving suburbs to play a much larger role in a new six years federal transportation bill to approved this year. In previous bill,80 percent of appropriations have by law gone to highways and only 20 percent to other transports. There many good reason why they should do this.    '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train.head(1)\n",
    "removeHTML(sample['full_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c178831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'many people have car where they live. the thing they don\\'t know is that when you use a car alot of thing can happen like you can get in accidet or the smoke that the car has is bad to breath on if someone is walk but in vauban,germany they dont have that proble because percent of vauban\\'s families do not own cars,and percent sold a car to move there. street parkig ,driveways and home garages are forbidden on the outskirts of freiburd that near the french and swiss borders. you probaly won\\'t see a car in vauban\\'s streets because they are completely \"car free\" but if some that lives in vauban that owns a car ownership is allowed,but there are only two places that you can park a large garages at the edge of the development,where a car owner buys a space but it not cheap to buy one they sell the space for you car for $, along with a home. the vauban people completed this in ,they said that this an example of a growing trend in europe,the untile states and some where else are suburban life from auto use this is called \"smart planning\". the current efforts to drastically reduce greenhouse gas emissions from tailes the passengee cars are responsible for percent of greenhouse gas emissions in europe and up to percent in some car intensive in the united states. i honeslty think that good idea that they did that is vaudan because that makes cities denser and better for walking and in vauban there are , residents within a rectangular square mile. in the artical david gold berg said that \"all of our development since world war has been centered on the cars,and that will have to change\" and i think that was very true what david gold said because alot thing we need cars to do we can go anyway were with out cars beacuse some people are a very lazy to walk to place thats why they alot of people use car and i think that it was a good idea that that they did that in vauban so people can see how we really don\\'t need car to go to place from place because we can walk from were we need to go or we can ride bycles with out the use of a car. it good that they are doing that if you thik about your help the earth in way and thats a very good thing to. in the united states ,the environmental protection agency is promoting what is called \"car reduced\"communtunties,and the legislators are starting to act,if cautiously. maany experts expect pubic transport serving suburbs to play a much larger role in a new six years federal transportation bill to approved this year. in previous bill, percent of appropriations have by law gone to highways and only percent to other transports. there many good reason why they should do this.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPreprocessing(sample['full_text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf23ec",
   "metadata": {
    "papermill": {
     "duration": 0.0083,
     "end_time": "2024-04-09T08:00:05.098535",
     "exception": false,
     "start_time": "2024-04-09T08:00:05.090235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.Paragraph Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccf1cacf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T08:00:05.119090Z",
     "iopub.status.busy": "2024-04-09T08:00:05.118582Z",
     "iopub.status.idle": "2024-04-09T08:00:13.593393Z",
     "shell.execute_reply": "2024-04-09T08:00:13.592518Z"
    },
    "papermill": {
     "duration": 8.489147,
     "end_time": "2024-04-09T08:00:13.596320",
     "exception": false,
     "start_time": "2024-04-09T08:00:05.107173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>paragraph_len_min</th>\n",
       "      <th>paragraph_sentence_cnt_min</th>\n",
       "      <th>paragraph_word_cnt_min</th>\n",
       "      <th>paragraph_len_first</th>\n",
       "      <th>paragraph_sentence_cnt_first</th>\n",
       "      <th>paragraph_word_cnt_first</th>\n",
       "      <th>paragraph_len_last</th>\n",
       "      <th>paragraph_sentence_cnt_last</th>\n",
       "      <th>paragraph_word_cnt_last</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2640</td>\n",
       "      <td>14</td>\n",
       "      <td>491</td>\n",
       "      <td>2640</td>\n",
       "      <td>14</td>\n",
       "      <td>491</td>\n",
       "      <td>2640</td>\n",
       "      <td>14</td>\n",
       "      <td>491</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>184</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>184</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>235</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>476</td>\n",
       "      <td>5</td>\n",
       "      <td>85</td>\n",
       "      <td>576</td>\n",
       "      <td>5</td>\n",
       "      <td>101</td>\n",
       "      <td>476</td>\n",
       "      <td>5</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  paragraph_len_min  \\\n",
       "0                  1                  1  ...               2640   \n",
       "1                  3                  3  ...                184   \n",
       "2                  4                  4  ...                476   \n",
       "\n",
       "   paragraph_sentence_cnt_min  paragraph_word_cnt_min  paragraph_len_first  \\\n",
       "0                          14                     491                 2640   \n",
       "1                           3                      37                  184   \n",
       "2                           5                      85                  576   \n",
       "\n",
       "   paragraph_sentence_cnt_first  paragraph_word_cnt_first  paragraph_len_last  \\\n",
       "0                            14                       491                2640   \n",
       "1                             4                        37                 235   \n",
       "2                             5                       101                 476   \n",
       "\n",
       "   paragraph_sentence_cnt_last  paragraph_word_cnt_last  score  \n",
       "0                           14                      491      3  \n",
       "1                            3                       46      3  \n",
       "2                            5                       85      4  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 段落特征\n",
    "# paragraph features\n",
    "def Paragraph_Preprocess(tmp):\n",
    "    # 将段落列表扩展为一行行的数据\n",
    "    # Expand the paragraph list into several lines of data\n",
    "    tmp = tmp.explode('paragraph')\n",
    "    # 段落预处理\n",
    "    # Paragraph preprocessing\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').apply(dataPreprocessing))\n",
    "    # 计算每一个段落的长度\n",
    "    # Calculate the length of each paragraph\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').apply(lambda x: len(x)).alias(\"paragraph_len\"))\n",
    "    # 计算每一个段落中句子的数量和单词的数量\n",
    "    # Calculate the number of sentences and words in each paragraph\n",
    "    tmp = tmp.with_columns([\n",
    "    pl.col('paragraph').apply(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n",
    "    pl.col('paragraph').apply(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),\n",
    "])\n",
    "    return tmp\n",
    "# feature_eng\n",
    "paragraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n",
    "def Paragraph_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        # 统计段落长度大于和小于 i 值的个数\n",
    "        # Count the number of paragraph lengths greater than and less than the i-value\n",
    "        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n",
    "        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n",
    "        # 其他\n",
    "        # other\n",
    "        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n",
    "        ]\n",
    "    df = train_tmp.groupby(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "tmp = Paragraph_Preprocess(train)\n",
    "train_feats = Paragraph_Eng(tmp)\n",
    "train_feats['score'] = train['score']\n",
    "# 获取特征名称\n",
    "# Obtain feature names\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8414edd",
   "metadata": {
    "papermill": {
     "duration": 0.008634,
     "end_time": "2024-04-09T08:00:13.614144",
     "exception": false,
     "start_time": "2024-04-09T08:00:13.605510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.Sentence Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb27a591",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T08:00:13.635983Z",
     "iopub.status.busy": "2024-04-09T08:00:13.635170Z",
     "iopub.status.idle": "2024-04-09T08:00:21.794967Z",
     "shell.execute_reply": "2024-04-09T08:00:21.793439Z"
    },
    "papermill": {
     "duration": 8.173991,
     "end_time": "2024-04-09T08:00:21.797622",
     "exception": false,
     "start_time": "2024-04-09T08:00:13.623631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>sentence_len_max</th>\n",
       "      <th>sentence_word_cnt_max</th>\n",
       "      <th>sentence_len_mean</th>\n",
       "      <th>sentence_word_cnt_mean</th>\n",
       "      <th>sentence_len_min</th>\n",
       "      <th>sentence_word_cnt_min</th>\n",
       "      <th>sentence_len_first</th>\n",
       "      <th>sentence_word_cnt_first</th>\n",
       "      <th>sentence_len_last</th>\n",
       "      <th>sentence_word_cnt_last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>593</td>\n",
       "      <td>127</td>\n",
       "      <td>202.076923</td>\n",
       "      <td>38.692308</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>250</td>\n",
       "      <td>49</td>\n",
       "      <td>96.823529</td>\n",
       "      <td>20.470588</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>62</td>\n",
       "      <td>13</td>\n",
       "      <td>124</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>237</td>\n",
       "      <td>47</td>\n",
       "      <td>126.708333</td>\n",
       "      <td>23.875000</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>144</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  sentence_len_max  \\\n",
       "0                  1                  1  ...               593   \n",
       "1                  3                  3  ...               250   \n",
       "2                  4                  4  ...               237   \n",
       "\n",
       "   sentence_word_cnt_max  sentence_len_mean  sentence_word_cnt_mean  \\\n",
       "0                    127         202.076923               38.692308   \n",
       "1                     49          96.823529               20.470588   \n",
       "2                     47         126.708333               23.875000   \n",
       "\n",
       "   sentence_len_min  sentence_word_cnt_min  sentence_len_first  \\\n",
       "0                36                      7                  36   \n",
       "1                27                      7                  62   \n",
       "2                58                     10                 144   \n",
       "\n",
       "   sentence_word_cnt_first  sentence_len_last  sentence_word_cnt_last  \n",
       "0                        7                 47                      10  \n",
       "1                       13                124                      25  \n",
       "2                       27                 58                      10  \n",
       "\n",
       "[3 rows x 50 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence feature\n",
    "def Sentence_Preprocess(tmp):\n",
    "    # 对full_text预处理，并且使用句号分割出文本的句子\n",
    "    # Preprocess full_text and use periods to segment sentences in the text\n",
    "    tmp = tmp.with_columns(pl.col('full_text').apply(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n",
    "    tmp = tmp.explode('sentence')\n",
    "    # 计算句子的长度\n",
    "    # Calculate the length of a sentence\n",
    "    tmp = tmp.with_columns(pl.col('sentence').apply(lambda x: len(x)).alias(\"sentence_len\"))\n",
    "    # 筛选出句子长度大于15的那一部分数据\n",
    "    # Filter out the portion of data with a sentence length greater than 15\n",
    "    tmp = tmp.filter(pl.col('sentence_len')>=15)\n",
    "    # 统计每一句中单词的数量\n",
    "    # Count the number of words in each sentence\n",
    "    tmp = tmp.with_columns(pl.col('sentence').apply(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n",
    "    \n",
    "    return tmp\n",
    "# feature_eng\n",
    "sentence_fea = ['sentence_len','sentence_word_cnt']\n",
    "def Sentence_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        # 统计句子长度大于 i 的句子个数\n",
    "        # Count the number of sentences with a length greater than i\n",
    "        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n",
    "        # 其他\n",
    "        # other\n",
    "        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n",
    "        ]\n",
    "    df = train_tmp.groupby(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "tmp = Sentence_Preprocess(train)\n",
    "# 将新生成的特征数据和之前生成的特征数据合并\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c20fe79",
   "metadata": {
    "papermill": {
     "duration": 0.009058,
     "end_time": "2024-04-09T08:00:21.816171",
     "exception": false,
     "start_time": "2024-04-09T08:00:21.807113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.Word Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "901cc1de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T08:00:21.837635Z",
     "iopub.status.busy": "2024-04-09T08:00:21.836955Z",
     "iopub.status.idle": "2024-04-09T08:00:37.320633Z",
     "shell.execute_reply": "2024-04-09T08:00:37.319247Z"
    },
    "papermill": {
     "duration": 15.497665,
     "end_time": "2024-04-09T08:00:37.323416",
     "exception": false,
     "start_time": "2024-04-09T08:00:21.825751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word feature\n",
    "def Word_Preprocess(tmp):\n",
    "    # 对full_text预处理，并且使用空格符分割出文本的单词\n",
    "    # Preprocess full_text and use spaces to separate words from the text\n",
    "    tmp = tmp.with_columns(pl.col('full_text').apply(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n",
    "    tmp = tmp.explode('word')\n",
    "    # 计算每一个的单词长度\n",
    "    # Calculate the length of each word\n",
    "    tmp = tmp.with_columns(pl.col('word').apply(lambda x: len(x)).alias(\"word_len\"))\n",
    "    # 删除单词长度为0的数据\n",
    "    # Delete data with a word length of 0\n",
    "    tmp = tmp.filter(pl.col('word_len')!=0)\n",
    "    \n",
    "    return tmp\n",
    "# feature_eng\n",
    "def Word_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        # 统计单词长度大于 i+1 的单词个数\n",
    "        # Count the number of words with a length greater than i+1\n",
    "        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n",
    "        # 其他\n",
    "        # other\n",
    "        pl.col('word_len').max().alias(f\"word_len_max\"),\n",
    "        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n",
    "        pl.col('word_len').std().alias(f\"word_len_std\"),\n",
    "        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n",
    "        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n",
    "        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n",
    "        ]\n",
    "    df = train_tmp.groupby(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "tmp = Word_Preprocess(train)\n",
    "# 将新生成的特征数据和之前生成的特征数据合并\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0e500",
   "metadata": {
    "papermill": {
     "duration": 0.00972,
     "end_time": "2024-04-09T08:00:37.343157",
     "exception": false,
     "start_time": "2024-04-09T08:00:37.333437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.Tf-idf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a79a23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T08:00:37.365523Z",
     "iopub.status.busy": "2024-04-09T08:00:37.364528Z",
     "iopub.status.idle": "2024-04-09T08:02:19.724143Z",
     "shell.execute_reply": "2024-04-09T08:02:19.722893Z"
    },
    "papermill": {
     "duration": 102.385103,
     "end_time": "2024-04-09T08:02:19.738282",
     "exception": false,
     "start_time": "2024-04-09T08:00:37.353179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  3360\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>tfid_3281</th>\n",
       "      <th>tfid_3282</th>\n",
       "      <th>tfid_3283</th>\n",
       "      <th>tfid_3284</th>\n",
       "      <th>tfid_3285</th>\n",
       "      <th>tfid_3286</th>\n",
       "      <th>tfid_3287</th>\n",
       "      <th>tfid_3288</th>\n",
       "      <th>tfid_3289</th>\n",
       "      <th>tfid_3290</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034738</td>\n",
       "      <td>0.071064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3362 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  tfid_3281  tfid_3282  tfid_3283  \\\n",
       "0                  1                  1  ...        0.0        0.0        0.0   \n",
       "1                  3                  3  ...        0.0        0.0        0.0   \n",
       "2                  4                  4  ...        0.0        0.0        0.0   \n",
       "\n",
       "   tfid_3284  tfid_3285  tfid_3286  tfid_3287  tfid_3288  tfid_3289  tfid_3290  \n",
       "0        0.0        0.0        0.0   0.034738   0.071064        0.0        0.0  \n",
       "1        0.0        0.0        0.0   0.000000   0.000000        0.0        0.0  \n",
       "2        0.0        0.0        0.0   0.000000   0.000000        0.0        0.0  \n",
       "\n",
       "[3 rows x 3362 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer parameter\n",
    "vectorizer = TfidfVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(1,3),\n",
    "            min_df=0.05,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True,\n",
    ")\n",
    "# 将全部数据集都填充进TfidfVectorizer里，这可能会造成泄露和过于乐观的CV分数\n",
    "# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\n",
    "train_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n",
    "# 转换为数组\n",
    "# Convert to array\n",
    "dense_matrix = train_tfid.toarray()\n",
    "# 转换为dataframe\n",
    "# Convert to dataframe\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "# 重命名特征\n",
    "# rename features\n",
    "tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = train_feats['essay_id']\n",
    "# 将新生成的特征数据和之前生成的特征数据合并\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(df, on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799ddd2",
   "metadata": {
    "papermill": {
     "duration": 0.010114,
     "end_time": "2024-04-09T08:02:19.758644",
     "exception": false,
     "start_time": "2024-04-09T08:02:19.748530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train\n",
    "* I have trained and saved the model\n",
    "* you can choose to retrain or load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7afcf58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T08:02:19.782156Z",
     "iopub.status.busy": "2024-04-09T08:02:19.781752Z",
     "iopub.status.idle": "2024-04-09T08:02:19.790738Z",
     "shell.execute_reply": "2024-04-09T08:02:19.789524Z"
    },
    "papermill": {
     "duration": 0.023996,
     "end_time": "2024-04-09T08:02:19.793287",
     "exception": false,
     "start_time": "2024-04-09T08:02:19.769291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    y_true = y_true + a\n",
    "    y_pred = (y_pred + a).clip(1, 6).round()\n",
    "    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "    return 'QWK', qwk, True\n",
    "def qwk_obj(y_true, y_pred):\n",
    "    labels = y_true + a\n",
    "    preds = y_pred + a\n",
    "    preds = preds.clip(1, 6)\n",
    "    f = 1/2*np.sum((preds-labels)**2)\n",
    "    g = 1/2*np.sum((preds-a)**2+b)\n",
    "    df = preds - labels\n",
    "    dg = preds - a\n",
    "    grad = (df/g - f*dg/g**2)*len(labels)\n",
    "    hess = np.ones(len(labels))\n",
    "    return grad, hess\n",
    "a = 2.948\n",
    "b = 1.092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d147d361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T08:02:19.816918Z",
     "iopub.status.busy": "2024-04-09T08:02:19.816469Z",
     "iopub.status.idle": "2024-04-09T08:02:19.965371Z",
     "shell.execute_reply": "2024-04-09T08:02:19.964122Z"
    },
    "papermill": {
     "duration": 0.164359,
     "end_time": "2024-04-09T08:02:19.968564",
     "exception": false,
     "start_time": "2024-04-09T08:02:19.804205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOAD = True\n",
    "models = []\n",
    "if LOAD:\n",
    "    for i in range(5):\n",
    "        models.append(lgb.Booster(model_file=f'../input/lal-lgb-baseline-4/fold_{i}.txt'))\n",
    "else:\n",
    "    # oof用于存储每一次模型对验证集的预测结果\n",
    "    # OOF is used to store the prediction results of each model on the validation set\n",
    "    oof = []\n",
    "    x= train_feats\n",
    "    y= train_feats['score'].values\n",
    "    # 5 fold\n",
    "    kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n",
    "    for fold_id, (trn_idx, val_idx) in tqdm(enumerate(kfold.split(x.copy(), y.copy().astype(str)))):\n",
    "            # 创建模型\n",
    "            # create model\n",
    "            model = lgb.LGBMRegressor(\n",
    "                objective = qwk_obj,\n",
    "                metrics = 'None',\n",
    "                learning_rate = 0.1,\n",
    "                max_depth = 5,\n",
    "                num_leaves = 10,\n",
    "                colsample_bytree=0.5,\n",
    "                reg_alpha = 0.1,\n",
    "                reg_lambda = 0.8,\n",
    "                n_estimators=1024,\n",
    "                random_state=42,\n",
    "                verbosity = - 1)\n",
    "            # 分别取出5 kfold分割的训练集和验证集\n",
    "            # Take out the training and validation sets for 5 kfold segmentation separately\n",
    "            X_train = train_feats.iloc[trn_idx][feature_names]\n",
    "            Y_train = train_feats.iloc[trn_idx]['score'] - a\n",
    "\n",
    "            X_val = train_feats.iloc[val_idx][feature_names]\n",
    "            Y_val = train_feats.iloc[val_idx]['score'] - a\n",
    "            print('\\nFold_{} Training ================================\\n'.format(fold_id+1))\n",
    "            # 训练模型\n",
    "            # Training model\n",
    "            lgb_model = model.fit(X_train,\n",
    "                                  Y_train,\n",
    "                                  eval_names=['train', 'valid'],\n",
    "                                  eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "                                  eval_metric=quadratic_weighted_kappa,\n",
    "                                  callbacks=callbacks,)\n",
    "            # 使用训练完成的模型对验证集进行预测\n",
    "            # Use the trained model to predict the validation set\n",
    "            pred_val = lgb_model.predict(\n",
    "                X_val, num_iteration=lgb_model.best_iteration_)\n",
    "            df_tmp = train_feats.iloc[val_idx][['essay_id', 'score']].copy()\n",
    "            df_tmp['pred'] = pred_val + a\n",
    "            oof.append(df_tmp)\n",
    "            # 保存模型参数\n",
    "            # Save model parameters\n",
    "            models.append(model.booster_)\n",
    "            lgb_model.booster_.save_model(f'fold_{fold_id}.txt')\n",
    "    df_oof = pd.concat(oof)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428c626",
   "metadata": {
    "papermill": {
     "duration": 0.009926,
     "end_time": "2024-04-09T08:02:19.988953",
     "exception": false,
     "start_time": "2024-04-09T08:02:19.979027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5c9b34b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T08:02:20.012863Z",
     "iopub.status.busy": "2024-04-09T08:02:20.012359Z",
     "iopub.status.idle": "2024-04-09T08:02:20.020541Z",
     "shell.execute_reply": "2024-04-09T08:02:20.019215Z"
    },
    "papermill": {
     "duration": 0.023922,
     "end_time": "2024-04-09T08:02:20.023128",
     "exception": false,
     "start_time": "2024-04-09T08:02:19.999206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.6275495464263015\n",
      "kappa:  0.7990509565910948\n"
     ]
    }
   ],
   "source": [
    "if LOAD:\n",
    "    print('acc: ',0.6275495464263015)\n",
    "    print('kappa: ',0.7990509565910948)\n",
    "else:\n",
    "    acc = accuracy_score(df_oof['score'], df_oof['pred'].clip(1, 6).round())\n",
    "    kappa = cohen_kappa_score(df_oof['score'], df_oof['pred'].clip(1, 6).round(), weights=\"quadratic\")\n",
    "    print('acc: ',acc)\n",
    "    print('kappa: ',kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962f729",
   "metadata": {
    "papermill": {
     "duration": 0.010035,
     "end_time": "2024-04-09T08:02:20.043593",
     "exception": false,
     "start_time": "2024-04-09T08:02:20.033558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c8b39af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T08:02:20.066960Z",
     "iopub.status.busy": "2024-04-09T08:02:20.066507Z",
     "iopub.status.idle": "2024-04-09T08:02:20.309531Z",
     "shell.execute_reply": "2024-04-09T08:02:20.307829Z"
    },
    "papermill": {
     "duration": 0.257913,
     "end_time": "2024-04-09T08:02:20.312322",
     "exception": false,
     "start_time": "2024-04-09T08:02:20.054409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features number:  3360\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>tfid_3281</th>\n",
       "      <th>tfid_3282</th>\n",
       "      <th>tfid_3283</th>\n",
       "      <th>tfid_3284</th>\n",
       "      <th>tfid_3285</th>\n",
       "      <th>tfid_3286</th>\n",
       "      <th>tfid_3287</th>\n",
       "      <th>tfid_3288</th>\n",
       "      <th>tfid_3289</th>\n",
       "      <th>tfid_3290</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034738</td>\n",
       "      <td>0.071064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3361 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  tfid_3281  tfid_3282  tfid_3283  \\\n",
       "0                  1                  1  ...        0.0        0.0        0.0   \n",
       "1                  3                  3  ...        0.0        0.0        0.0   \n",
       "2                  4                  4  ...        0.0        0.0        0.0   \n",
       "\n",
       "   tfid_3284  tfid_3285  tfid_3286  tfid_3287  tfid_3288  tfid_3289  tfid_3290  \n",
       "0        0.0        0.0        0.0   0.034738   0.071064        0.0        0.0  \n",
       "1        0.0        0.0        0.0   0.000000   0.000000        0.0        0.0  \n",
       "2        0.0        0.0        0.0   0.000000   0.000000        0.0        0.0  \n",
       "\n",
       "[3 rows x 3361 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paragraph\n",
    "tmp = Paragraph_Preprocess(test)\n",
    "test_feats = Paragraph_Eng(tmp)\n",
    "# Sentence\n",
    "tmp = Sentence_Preprocess(test)\n",
    "test_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n",
    "# Word\n",
    "tmp = Word_Preprocess(test)\n",
    "test_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n",
    "# Tfidf\n",
    "test_tfid = vectorizer.transform([i for i in test['full_text']])\n",
    "dense_matrix = test_tfid.toarray()\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = test_feats['essay_id']\n",
    "test_feats = test_feats.merge(df, on='essay_id', how='left')\n",
    "# Features number\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\n",
    "print('Features number: ',len(feature_names))\n",
    "test_feats.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "103739b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T08:02:20.339324Z",
     "iopub.status.busy": "2024-04-09T08:02:20.338601Z",
     "iopub.status.idle": "2024-04-09T08:02:20.431756Z",
     "shell.execute_reply": "2024-04-09T08:02:20.430309Z"
    },
    "papermill": {
     "duration": 0.110011,
     "end_time": "2024-04-09T08:02:20.434841",
     "exception": false,
     "start_time": "2024-04-09T08:02:20.324830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0644339  2.98101418 4.59023141]\n"
     ]
    }
   ],
   "source": [
    "prediction = test_feats[['essay_id']].copy()\n",
    "prediction['score'] = 0\n",
    "pred_test = models[0].predict(test_feats[feature_names]) + a\n",
    "for i in range(4):\n",
    "    pred_now = models[i+1].predict(test_feats[feature_names]) + a\n",
    "    pred_test = np.add(pred_test,pred_now)\n",
    "# 最终预测结果需要除以5，因为使用了5个模型的预测结果相加\n",
    "# The final prediction result needs to be divided by 5 because the prediction results of 5 models were added together\n",
    "pred_test = pred_test/5\n",
    "print(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "208cbf3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T08:02:20.459360Z",
     "iopub.status.busy": "2024-04-09T08:02:20.458270Z",
     "iopub.status.idle": "2024-04-09T08:02:20.475488Z",
     "shell.execute_reply": "2024-04-09T08:02:20.474670Z"
    },
    "papermill": {
     "duration": 0.032016,
     "end_time": "2024-04-09T08:02:20.477841",
     "exception": false,
     "start_time": "2024-04-09T08:02:20.445825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  score\n",
       "0  000d118    2.0\n",
       "1  000fe60    3.0\n",
       "2  001ab80    5.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将预测结果四舍五入为整数，限定范围为1-6（文章评分范围）\n",
    "# Round the prediction result to an integer and limit it to a range of 1-6 (score range)\n",
    "pred_test = pred_test.clip(1, 6).round()\n",
    "prediction['score'] = pred_test\n",
    "prediction.to_csv('submission.csv', index=False)\n",
    "prediction.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a24b1",
   "metadata": {
    "papermill": {
     "duration": 0.0112,
     "end_time": "2024-04-09T08:02:20.500595",
     "exception": false,
     "start_time": "2024-04-09T08:02:20.489395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reference Notebook\n",
    "#### I would like to give thanks to the authors of these public notebooks. I have learned a lot from you.\n",
    "* https://www.kaggle.com/code/davidjlochner/base-tfidf-lgbm\n",
    "* https://www.kaggle.com/code/yunsuxiaozi/aes2-0-baseline-naivebayesclassifier\n",
    "* https://www.kaggle.com/code/finlay/llm-detect-0-to-1\n",
    "* https://www.kaggle.com/code/awqatak/silver-bullet-single-model-165-features\n",
    "* https://www.kaggle.com/code/hiarsl/feature-engineering-sentence-paragraph-features\n",
    "* https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 4762179,
     "sourceId": 8070853,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 144.667811,
   "end_time": "2024-04-09T08:02:21.435351",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-09T07:59:56.767540",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
